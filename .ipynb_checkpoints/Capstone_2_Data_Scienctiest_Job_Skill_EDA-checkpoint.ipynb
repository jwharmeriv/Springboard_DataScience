{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "161939bb-562e-498a-a0ec-be7ef2401c19",
   "metadata": {},
   "source": [
    "Data Scientist and Data Analyst jobs require a specialized set of skills. I want to identify the most in demand skills in the Glassdoor job descriptions for a Data Scientist and Data Analyst, so I know which ones  to focus on developing first to be a more highly rated candidate during my job search. Understanding where these jobs are located and in which industries will allow me to more efficiently focus my search in the appropriate state and industry to increase my odds of getting a job as a Data Scientist or Data Analyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba55f6e8-81e2-4299-a8d3-8835324b7dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas and Numpy.\n",
    "# Load data from .CSV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "df = pd.read_csv('/Users/johnharmer/Library/CloudStorage/GoogleDrive-jwharmeriv@gmail.com/My Drive/Springboard - Data Science/Capstone Projects/Capstone 2/glassdoor_jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a390ec-944a-4548-bf3a-9a16c8dbe44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26792a8e-1745-4fe4-9c08-4741287b64f6",
   "metadata": {},
   "source": [
    "<h1 style=\"color: blue;\"><b>DATA CLEANING</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c21b97-24ca-43ec-9ce8-99a55bbe5967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter job titles for 'Data Scientist' or 'Data Analyst'\n",
    "df_filtered = df[df['Job Title'].str.contains('Data Scientist|Data Analyst', case=False, na=False)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1567b571-12ce-484d-a6e2-a0bbdc70eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter job titles for 'Data Scientist' or 'Data Analyst'\n",
    "df_filtered.loc[:, 'State'] = df_filtered['Location'].apply(\n",
    "    lambda x: 'Remote' if 'remote' in x.lower() else x.split(',')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc86d2-6235-48fb-bdd9-3668492273ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the state from the Location column, treating \"Remote\" as its own category\n",
    "df_filtered['State'] = df_filtered['Location'].apply(lambda x: 'Remote' if 'remote' in x.lower() else x.split(',')[-1].strip())\n",
    "\n",
    "# Mapping of full state names to their 2-letter abbreviations\n",
    "state_abbrev = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA', 'Colorado': 'CO', \n",
    "    'Connecticut': 'CT', 'Delaware': 'DE', 'District of Columbia': 'DC', 'Florida': 'FL', 'Georgia': 'GA', \n",
    "    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS', \n",
    "    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD', 'Massachusetts': 'MA', \n",
    "    'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO', 'Montana': 'MT', \n",
    "    'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', \n",
    "    'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK', \n",
    "    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC', 'South Dakota': 'SD', \n",
    "    'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', \n",
    "    'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'\n",
    "}\n",
    "\n",
    "# Function to map full state names to abbreviations\n",
    "def map_state_to_abbreviation(state):\n",
    "    state = state.strip()\n",
    "    if state in state_abbrev:\n",
    "        return state_abbrev[state]\n",
    "    return state\n",
    "\n",
    "# Drop unwanted entries\n",
    "df_filtered = df_filtered[~df_filtered['State'].isin(['United States', '-1', 'Point Loma', 'New York State', 'Manhattan'])]\n",
    "\n",
    "# Apply the state abbreviation mapping (keeping 'Remote' as is)\n",
    "df_filtered['State'] = df_filtered['State'].apply(lambda x: x if x == 'Remote' else map_state_to_abbreviation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0307dcf-6bbc-4fcd-a195-558debef31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d4280b-5a55-4d31-b7ae-6c0d74e1bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common skills, technologies, and software to look for in job descriptions\n",
    "skills_list = ['Python', 'SQL', 'Excel', 'R', 'Tableau', 'Power BI', 'Machine Learning', \n",
    "               'Deep Learning', 'Statistics', 'Data Mining', 'Hadoop', 'Spark', \n",
    "               'TensorFlow', 'Keras', 'scikit-learn', 'NLP', 'AWS', 'Azure', 'Cloud', 'Data Analysis']\n",
    "\n",
    "# Define a function to extract skills from the job descriptions\n",
    "def extract_skills(description):\n",
    "    found_skills = set()\n",
    "    for skill in skills_list:\n",
    "        if re.search(r'\\b' + re.escape(skill) + r'\\b', description, re.IGNORECASE):\n",
    "            found_skills.add(skill)\n",
    "    return ', '.join(found_skills)\n",
    "\n",
    "# Apply the function to the Job Description column and create a new 'Extracted Skills' column\n",
    "df_filtered['Extracted Skills'] = df_filtered['Job Description'].apply(extract_skills)\n",
    "\n",
    "# Display the updated dataframe with the new 'Extracted Skills' column\n",
    "print(df_filtered[['Job Title', 'State', 'Extracted Skills']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f63f7c0-20fb-4d98-b2a4-1845c3cff0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the extracted skills into one list\n",
    "all_skills = df_filtered['Extracted Skills'].str.split(', ').explode()\n",
    "\n",
    "# Count the occurrences of each skill\n",
    "skill_counts = Counter(all_skills)\n",
    "\n",
    "# Convert the Counter object to a DataFrame for easier plotting\n",
    "skills_df = pd.DataFrame(skill_counts.items(), columns=['Skill', 'Count']).sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Display the skill counts to the user\n",
    "print(skills_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c5192-56fa-43e7-b5b7-76e0a859ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty skill entries and plot the skill counts\n",
    "skills_df = skills_df[skills_df['Skill'] != '']\n",
    "\n",
    "# Create bar plot to show Skills count\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(skills_df['Skill'], skills_df['Count'], color='skyblue')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Skill')\n",
    "plt.title('Skill Frequency in Job Descriptions')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeaf7f4-1dc5-4f03-a29a-475b467b960b",
   "metadata": {},
   "source": [
    "<b><h3>Most In-Demand Skills</b></h3>\n",
    "<b>Top 15 Skills:</b> Machine Learning, Python, Statistics, R, Data Analysis, Data Mining, SQL, Cloud, Tableau, NLP, Excel, Deep Learning, Spark, TensorFlow, AWS<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ab613-9179-48e2-b037-9f654c822218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many unique job titles are in the df.\n",
    "job_titles = df['Job Title'].value_counts()\n",
    "print(job_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded8178-0b58-443d-a8f7-c824713e496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of 'Data Scientist' and 'Data Analyst' separately\n",
    "data_scientist_count = df_filtered['Job Title'].str.contains('Data Scientist', case=False, na=False).sum()\n",
    "data_analyst_count = df_filtered['Job Title'].str.contains('Data Analyst', case=False, na=False).sum()\n",
    "\n",
    "# Display counts\n",
    "print(data_scientist_count, data_analyst_count)\n",
    "\n",
    "# Create a DataFrame to hold the counts for plotting\n",
    "job_title_data = pd.DataFrame({\n",
    "    'Job Title': ['Data Scientist', 'Data Analyst'],\n",
    "    'Count': [data_scientist_count, data_analyst_count]})\n",
    "\n",
    "# Create a bar plot to visualize the counts\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(job_title_data['Job Title'], job_title_data['Count'], color=['skyblue', 'lightgreen'])\n",
    "plt.ylabel('Number of Positions')\n",
    "plt.title('Count of Job Titles Containing \"Data Scientist\" or \"Data Analyst\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a033de1f-8f5a-4e74-bb8e-eb3f2969bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique industries in the df_filtered.\n",
    "unique_industries_count = df_filtered['Industry'].nunique()\n",
    "\n",
    "unique_industries_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5676067-bb13-4508-8076-3b676233bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each Industry.\n",
    "unique_industries_count = df_filtered['Industry'].value_counts()\n",
    "unique_industries_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcaaf0d-8402-4ee6-8f3e-7e24f9fb32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove industries where the value is '-1'\n",
    "industry_counts_cleaned = unique_industries_count[unique_industries_count.index != '-1']\n",
    "\n",
    "# Print the cleaned industry counts with the industry names as the index\n",
    "print(industry_counts_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d449678e-b656-429e-ab96-30f118062244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the counts of each industry to help identify which industry is more likely to be hiring.\n",
    "plt.figure(figsize=(10, 10))\n",
    "industry_counts_cleaned.plot(kind='barh', color='lightblue')\n",
    "plt.xlabel('Number of Positions')\n",
    "plt.ylabel('Industry')\n",
    "plt.title('Industry Hiring Counts')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for readability\n",
    "plt.grid(True, which='both', axis='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398000a-b3c7-4240-b9dd-53e61dfe818a",
   "metadata": {},
   "source": [
    "<b><h3>RESULTS</b></h3>\n",
    "1226 jobs with job titles containing Data Scientist(1192) or Data Analyst(32).<br>\n",
    "\n",
    "<b><h4>Top Industries Hiring:</b></h4>\n",
    "Information Technology, Finance, Healthcare, Enterprise Software & Network Solutions, Aerospace & Defense.\n",
    "IT support services has the most job openings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5fafd8-6cca-481f-8519-dd7c53956358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many jobs are located in each state, including remote positions\n",
    "jobs_by_state = df_filtered['State'].value_counts()\n",
    "\n",
    "# Display the results using standard print\n",
    "print(jobs_by_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a13e45-a4ab-423e-8013-79a50ae6823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a bar chart to show job counts by state\n",
    "plt.figure(figsize=(10, 8))\n",
    "jobs_by_state.plot(kind='barh', color='skyblue')\n",
    "plt.xlabel('Number of Jobs')\n",
    "plt.ylabel('State')\n",
    "plt.title('Job Counts by State')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for readability\n",
    "plt.grid(True, which='both', axis='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff7ac19-49c6-4500-98c2-1e29ae258520",
   "metadata": {},
   "source": [
    "<h3><B>Top 10 states with job openings</B></h3>\n",
    "Remote    218<br>\n",
    "CA        195<br>\n",
    "VA        105<br>\n",
    "TX         77<br>\n",
    "NJ         77<br>\n",
    "MA         59<br>\n",
    "CO         39<br>\n",
    "PA         38<br>\n",
    "NC         36<br>\n",
    "MI         30<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e26724-6b55-4740-98fe-9e6158cda1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and parse the salary column data\n",
    "def clean_salary(salary):\n",
    "    if 'K' in salary:\n",
    "        salary = salary.replace('K', '').replace('$', '').replace('Employer Provided Salary:', '').replace('(Glassdoor est.)', '').strip()\n",
    "        if '-' in salary:\n",
    "            # Split into minimum and maximum salary and compute the average\n",
    "            min_salary, max_salary = salary.split('-')\n",
    "            avg_salary = (int(min_salary) + int(max_salary)) / 2\n",
    "            return avg_salary\n",
    "        else:\n",
    "            # Single salary value\n",
    "            return float(salary.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f68bce-af68-404a-86e1-d1839d7f9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the clean_salary function to the Salary Estimate column\n",
    "df_filtered['Avg Salary'] = df_filtered['Salary Estimate'].apply(clean_salary)\n",
    "\n",
    "# Fill NaNs with a placeholder value (e.g., 0) and convert to integers\n",
    "df_filtered['Avg Salary'].fillna(0, inplace=True)\n",
    "df_filtered['Avg Salary'] = df_filtered['Avg Salary'].astype(int)\n",
    "\n",
    "# Display 'Salary Estimate' and 'Avg Salary' to confirm.\n",
    "df_filtered[['Salary Estimate', 'Avg Salary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885ac4b-080c-4c00-bb39-1e93c3eec17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert revenue to integer\n",
    "def convert_to_integer(value):\n",
    "    if 'M' in value:\n",
    "        return int(float(value.replace('M', '').replace('+', '').replace('Less than ', '0.')) * 1e6)\n",
    "    elif 'B' in value:\n",
    "        return int(float(value.replace('B', '').replace('+', '').replace('Less than ', '0.')) * 1e9)\n",
    "    return int(value.replace('+', '').replace('Less than ', '0.'))\n",
    "\n",
    "# Function to parse and clean the revenue data with handling for mixed units and special cases\n",
    "def parse_revenue(revenue):\n",
    "    if 'Unknown' in revenue or 'Non-Applicable' in revenue:\n",
    "        return None\n",
    "    revenue = revenue.replace('(USD)', '').replace('$', '').replace(' million', 'M').replace(' billion', 'B').strip()\n",
    "    if ' to ' in revenue:\n",
    "        revenue_range = revenue.split(' to ')\n",
    "        if 'M' in revenue_range[0] and 'B' in revenue_range[1]:\n",
    "            min_revenue = float(revenue_range[0].replace('M', '').replace('+', '').replace('Less than ', '0.')) * 1e6\n",
    "            max_revenue = float(revenue_range[1].replace('B', '').replace('+', '').replace('Less than ', '0.')) * 1e9\n",
    "        elif 'B' in revenue_range[0] and 'M' in revenue_range[1]:\n",
    "            min_revenue = float(revenue_range[0].replace('B', '').replace('+', '').replace('Less than ', '0.')) * 1e9\n",
    "            max_revenue = float(revenue_range[1].replace('M', '').replace('+', '').replace('Less than ', '0.')) * 1e6\n",
    "        else:\n",
    "            min_revenue = convert_to_integer(revenue_range[0])\n",
    "            max_revenue = convert_to_integer(revenue_range[1])\n",
    "        return (min_revenue, max_revenue)\n",
    "    elif 'M' in revenue or 'B' in revenue:\n",
    "        revenue = convert_to_integer(revenue)\n",
    "        return (revenue, revenue)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2e6217-1ec4-418b-b680-4ea96fee185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the parsing function to the Revenue column\n",
    "df_filtered['parsed_revenue'] = df_filtered['Revenue'].apply(parse_revenue)\n",
    "\n",
    "df_filtered['parsed_revenue'] = df_filtered['Revenue'].apply(parse_revenue)\n",
    "\n",
    "# Extract min and max revenue\n",
    "df_filtered['Min Revenue'] = df_filtered['parsed_revenue'].apply(lambda x: x[0] if x is not None else None)\n",
    "df_filtered['Max Revenue'] = df_filtered['parsed_revenue'].apply(lambda x: x[1] if x is not None else None)\n",
    "\n",
    "# Drop the parsed_revenue column\n",
    "df_filtered.drop(columns=['parsed_revenue'], inplace=True)\n",
    "\n",
    "# Display the cleaned revenue ranges\n",
    "df_filtered[['Revenue', 'Min Revenue', 'Max Revenue']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432954e9-1d50-42c3-ac71-1df6b572e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics for numerical features\n",
    "summary_stats_numerical = df_filtered.describe()\n",
    "\n",
    "# Display summary statistics for categorical features\n",
    "summary_stats_categorical = df_filtered.describe(include=['O'])\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df_filtered.isnull().sum()\n",
    "\n",
    "summary_stats_numerical, summary_stats_categorical, missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1650bb3-4499-4650-ad9e-eaade768e013",
   "metadata": {},
   "source": [
    "<b><h3>Inferential Statistics</b></h3>\n",
    "<B>Hypotheses:</B><br>\n",
    "<b>H0 (Null Hypothesis):</b> There is no significant difference in the average salary estimate between industries.<br>\n",
    "<b>H1 (Alternative Hypothesis):</b> There is a significant difference in the average salary estimate between industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb066b-92b2-47ff-880f-da7dd9e4d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of average salary estimate vs. industry\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.scatterplot(data=df_filtered, x='Industry', y='Avg Salary')\n",
    "plt.title('Relationship Between Avg Salary Estimate and Industry')\n",
    "plt.xlabel('Industry')\n",
    "plt.ylabel('Avg Salary Estimate')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d91c654-981d-4051-a2ce-241d300b119e",
   "metadata": {},
   "source": [
    "Companies from the internet and software industires tend to pay higher wages. That being said, most of the industires are paying ~$100K or more for Data Scientist and Data Analyst jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de87219d-90c4-4a75-ac45-5fe60c87992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for box plot visualization\n",
    "job_ratings = df_filtered[df_filtered['Job Title'].str.contains('Data Scientist|Data Analyst', case=False, na=False)]\n",
    "\n",
    "# Box plot of Rating by job title\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.boxplot(x='Job Title', y='Rating', data=df_filtered)\n",
    "plt.title('Ratings by Job Title')\n",
    "plt.xlabel('Job Title')\n",
    "plt.ylabel('Company Rating')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b333046-3579-41dc-aa32-bbf9069e898a",
   "metadata": {},
   "source": [
    "The box plot shows the distribution of company ratings across all Data Scientist and Data Analyst jobs. Both roles have a similar distribution of ratings with the mean rating around 3.8. This indicates that companies hiring for these rols generally have good ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7168972-dd3f-48bb-bf9d-580a59fad3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of ratings by state\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(x='State', y='Rating', data=df_filtered)\n",
    "plt.title('Company Ratings by State')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Company Rating')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f79c2-751a-4f26-8118-612d68f5b368",
   "metadata": {},
   "source": [
    "The box plot shows the distribution of company ratings across all states with Data Scientist and Data Analyst roles available. Most states have median ratings of 3.5 to 4.1, indicating generally positive company ratings across the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a678a1b-8806-4808-a567-ef86d05260f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each industry\n",
    "industry_counts = df_filtered['Industry'].value_counts()\n",
    "\n",
    "# Filter for the top 10 industries\n",
    "top_10_industries = industry_counts.head(10).index\n",
    "\n",
    "# Filter the dataframe to include only rows from the top 10 industries\n",
    "df_top_10_industries = df_filtered[df_filtered['Industry'].isin(top_10_industries)]\n",
    "\n",
    "# Display the filtered dataframe with the top 10 industries\n",
    "df_top_10_industries[['Industry']].head()\n",
    "\n",
    "# Box plot of ratings by top industries\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Industry', y='Rating', data=df_top_10_industries)\n",
    "plt.title('Company Ratings by Industry')\n",
    "plt.xlabel('Industry')\n",
    "plt.ylabel('Company Rating')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee8b24-e086-4b91-bf54-d5a413bb3759",
   "metadata": {},
   "source": [
    "The box plot shows the distribution of company ratings across the top 10 industries. Most industries have median ratings around 3.8 to 4.0, with some variability in the distribution. This indicates that while most industries maintain good company ratings, there are some differences in the spread of these ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe56b0-ceb8-4726-aa58-06fbcc24bc3a",
   "metadata": {},
   "source": [
    "<b>Most In-Demand Skills:</b><br>\n",
    "<b>Top Skills:</b> Analytics, Machine learning, Analysis, AI, Modeling, Python, Statistics, Statistical analysis, AWS, SQL, Data visualization, SAS, Data analysis, Tableau, Data mining.<br>\n",
    "These skills are critical for candidates looking to secure a Data Scientist or Data Analyst position.<br><br>\n",
    "<b>Top Locations for Jobs:</b><br>\n",
    "<b>Top Locations:</b> Major states with tech hubs have the most job opportunities.<br>\n",
    "Visualization shows the distribution of these jobs across each state.<br><br>\n",
    "<b>Top Industries Hiring:</b><br>\n",
    "<b>Key Industries:</b> Information Technology, Finance, Healthcare, Enterprise Software & Network Solutions, and Aerospace & Defense.\n",
    "Visualization shows the distribution of these jobs across various industries.<br><br>\n",
    "<b>Data Relationships:</b><br>\n",
    "<b>Scatter Plot:</b> Weak positive correlation between average salary estimates and industry.<br>\n",
    "<b>Box Plots:</b> Similar distributions of company ratings across job titles, locations, and industries.<br><br>\n",
    "<b>Feature Engineering:</b><br>\n",
    "Categorical features have been one-hot encoded.<br>\n",
    "Numerical features have been standardized.<br><br>\n",
    "<b>Recommendations</b><br>\n",
    "<b>1. Skill Development:</b> Prioritize learning Machine learning, Analytics, Python, Statistics, SQL, and Data visualization. These are the some of the most frequently mentioned skills in job descriptions.<br>\n",
    "<b>2. Targeted Job Search:</b> Focus on the top locations and industries identified. This will increase the efficiency of your job search<br>\n",
    "<b>3. Continuous Analysis:</b> Regularly update the analysis to capture trends and changes in the job market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c571d3-d064-4609-9ccc-003bfb3188fa",
   "metadata": {},
   "source": [
    "<b>Build 3 different models to determine which is best.</b> 1. Logistic Regression with One-Hot Encoding, 2. Random Forest Classifier with Label Encoding, 3. Gradient Boosting (LightGBM, CatBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12056497-c3ef-473a-bea7-d9f4bfadd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all unique skills from the 'Extracted Skills' column\n",
    "unique_skills = set(skill for skills_list in df_filtered['Extracted Skills'].dropna().str.split(', ') for skill in skills_list)\n",
    "\n",
    "# Create binary columns for each skill (multi-label target)\n",
    "for skill in unique_skills:\n",
    "    df_filtered[f'Target_{skill}'] = df_filtered['Extracted Skills'].apply(lambda x: 1 if skill in x else 0)\n",
    "\n",
    "# Define the target variables (multi-label format)\n",
    "y = df_filtered[[f'Target_{skill}' for skill in unique_skills]]\n",
    "\n",
    "# Check and remove skills that only have one class (all 0s or all 1s)\n",
    "valid_skills = [col for col in y.columns if y[col].nunique() > 1]\n",
    "\n",
    "# Filter the target dataframe to only include valid skills (with both 0s and 1s)\n",
    "y_valid = y[valid_skills]\n",
    "\n",
    "# Define features (e.g., 'Rating', 'State', 'Industry')\n",
    "X = df_filtered[['Rating', 'State', 'Industry']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "X_encoded = pd.get_dummies(X, columns=['State', 'Industry'], drop_first=True)\n",
    "\n",
    "# Handle missing values in 'Rating' by filling with the mean\n",
    "X_encoded['Rating'].fillna(X_encoded['Rating'].mean(), inplace=True)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_valid, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a multi-output logistic regression model (one model per skill)\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "multi_target_model = MultiOutputClassifier(log_reg, n_jobs=-1)\n",
    "multi_target_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for each skill and evaluate the models\n",
    "y_pred = multi_target_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy for each skill\n",
    "accuracies = (y_pred == y_test).mean(axis=0)\n",
    "\n",
    "# Display the accuracy for each skill\n",
    "skill_accuracy = pd.DataFrame({'Skill': valid_skills, 'Accuracy': accuracies})\n",
    "print(skill_accuracy.sort_values(by='Accuracy', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb112bb-deb9-4c60-89ad-5af10c49aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# Initialize dictionaries to store the evaluation metrics for each skill\n",
    "evaluation_metrics = {\n",
    "    'Skill': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': [],\n",
    "    'Confusion Matrix': []\n",
    "}\n",
    "\n",
    "# Loop through each skill (target column) to compute the metrics\n",
    "for skill in valid_skills:\n",
    "    # Predictions and true values for the skill\n",
    "    y_pred_skill = y_pred[:, y_valid.columns.get_loc(skill)]\n",
    "    y_true_skill = y_test[skill].values\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true_skill, y_pred_skill)\n",
    "    \n",
    "    # Precision, Recall, F1 Score - set to zero if no positive predictions\n",
    "    precision = precision_score(y_true_skill, y_pred_skill, zero_division=0)\n",
    "    recall = recall_score(y_true_skill, y_pred_skill, zero_division=0)\n",
    "    f1 = f1_score(y_true_skill, y_pred_skill, zero_division=0)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true_skill, y_pred_skill)\n",
    "    \n",
    "    # Store the metrics\n",
    "    evaluation_metrics['Skill'].append(skill)\n",
    "    evaluation_metrics['Accuracy'].append(acc)\n",
    "    evaluation_metrics['Precision'].append(precision)\n",
    "    evaluation_metrics['Recall'].append(recall)\n",
    "    evaluation_metrics['F1 Score'].append(f1)\n",
    "    evaluation_metrics['Confusion Matrix'].append(cm)\n",
    "\n",
    "# Convert the metrics dictionary to a DataFrame for easy viewing\n",
    "evaluation_df = pd.DataFrame(evaluation_metrics)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(evaluation_df[['Skill', 'Accuracy', 'Precision', 'Recall', 'F1 Score']])\n",
    "\n",
    "# Print the confusion matrix for each skill\n",
    "for skill, cm in zip(evaluation_metrics['Skill'], evaluation_metrics['Confusion Matrix']):\n",
    "    print(f\"\\nConfusion Matrix for {skill}:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38d634-bee4-4951-8490-cf5883907dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Tune model for better performance\n",
    "# Define a grid of regularization strength values\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize a list to store the best models for each skill\n",
    "best_logistic_models = {}\n",
    "\n",
    "# Loop through each skill (multi-label target)\n",
    "for skill in valid_skills:\n",
    "    print(f\"Running GridSearch for {skill}...\")\n",
    "    \n",
    "    # Create a Logistic Regression instance\n",
    "    log_reg = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "    \n",
    "    # Set up GridSearchCV for the current skill (1D target)\n",
    "    grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='f1_macro')\n",
    "    \n",
    "    # Fit GridSearchCV for the current skill\n",
    "    grid_search.fit(X_train, y_train[skill])\n",
    "    \n",
    "    # Store the best model for the current skill\n",
    "    best_logistic_models[skill] = grid_search.best_estimator_\n",
    "    \n",
    "    # Print the best hyperparameters for the current skill\n",
    "    print(f\"Best parameters for {skill}: {grid_search.best_params_}\")\n",
    "\n",
    "# After the loop, we have the best logistic regression models for each skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f25f8c-3b37-4a25-8afd-53e98c4c8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best C values from the grid search\n",
    "best_params = {\n",
    "    'Target_Tableau': 0.1,\n",
    "    'Target_Hadoop': 100,\n",
    "    'Target_Deep Learning': 1,\n",
    "    'Target_Spark': 10,\n",
    "    'Target_TensorFlow': 100,\n",
    "    'Target_Machine Learning': 100,\n",
    "    'Target_scikit-learn': 100,\n",
    "    'Target_Data Analysis': 100,\n",
    "    'Target_Keras': 100,\n",
    "    'Target_Excel': 100,\n",
    "    'Target_AWS': 1,\n",
    "    'Target_SQL': 0.01,\n",
    "    'Target_Cloud': 10,\n",
    "    'Target_NLP': 1,\n",
    "    'Target_Data Mining': 0.1,\n",
    "    'Target_Power BI': 100,\n",
    "    'Target_R': 0.01,\n",
    "    'Target_Python': 100,\n",
    "    'Target_Azure': 1,\n",
    "    'Target_Statistics': 10\n",
    "}\n",
    "\n",
    "# Initialize the logistic regression models using the best parameters\n",
    "logistic_models = {}\n",
    "for skill, C_value in best_params.items():\n",
    "    logistic_models[skill] = LogisticRegression(C=C_value, max_iter=1000, class_weight='balanced')\n",
    "\n",
    "# Train each model on the respective target skill\n",
    "for skill, model in logistic_models.items():\n",
    "    print(f\"Training model for {skill}...\")\n",
    "    model.fit(X_train, y_train[skill])\n",
    "\n",
    "# Make predictions and evaluate the models for each skill\n",
    "evaluation_metrics = {'Skill': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': [], 'Confusion Matrix': []}\n",
    "\n",
    "for skill, model in logistic_models.items():\n",
    "    print(f\"Evaluating model for {skill}...\")\n",
    "    y_pred_skill = model.predict(X_test)\n",
    "    y_true_skill = y_test[skill].values\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    acc = accuracy_score(y_true_skill, y_pred_skill)\n",
    "    precision = precision_score(y_true_skill, y_pred_skill, zero_division=0)\n",
    "    recall = recall_score(y_true_skill, y_pred_skill, zero_division=0)\n",
    "    f1 = f1_score(y_true_skill, y_pred_skill, zero_division=0)\n",
    "    cm = confusion_matrix(y_true_skill, y_pred_skill)\n",
    "\n",
    "    # Store the metrics\n",
    "    evaluation_metrics['Skill'].append(skill)\n",
    "    evaluation_metrics['Accuracy'].append(acc)\n",
    "    evaluation_metrics['Precision'].append(precision)\n",
    "    evaluation_metrics['Recall'].append(recall)\n",
    "    evaluation_metrics['F1 Score'].append(f1)\n",
    "    evaluation_metrics['Confusion Matrix'].append(cm)\n",
    "\n",
    "# Convert the metrics dictionary to a DataFrame for easy viewing\n",
    "evaluation_df = pd.DataFrame(evaluation_metrics)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(evaluation_df[['Skill', 'Accuracy', 'Precision', 'Recall', 'F1 Score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312b2b1-9bee-4fd2-b085-f01147ab18f9",
   "metadata": {},
   "source": [
    "<h3>Summary:</h3>\n",
    "<b>Improvements:</b> Tuning the logistic regression model resulted in improved F1 scores for most skills, particularly those that had poor scores in the original model. Skills such as Target_Machine Learning, Target_Tableau, and Target_Statistics benefited the most from the tuning.\n",
    "\n",
    "<b>Challenges with Rare Skills:</b> Despite tuning, skills with fewer positive examples (e.g., Target_Keras, Target_Power BI) still exhibit poor performance due to class imbalance.\n",
    "\n",
    "<b>Trade-offs:</b> While tuning improved performance for many skills, some skills like Target_Data Analysis and Target_R saw slight decreases in performance, indicating a potential trade-off between precision and recall or overfitting in certain areas.\n",
    "\n",
    "To further improve, additional techniques like SMOTE (Synthetic Minority Over-sampling Technique) or undersampling for imbalanced classes could be applied, especially for skills with few positive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a963f92-464e-41ac-99d9-436d0d590771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier with label encoding\n",
    "# Instantiate the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Wrap it in MultiOutputClassifier to handle multi-label classification\n",
    "multi_rf_model = MultiOutputClassifier(rf_model, n_jobs=-1)\n",
    "\n",
    "# Train the Random Forest model on the training data\n",
    "multi_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for each skill on the test set\n",
    "y_pred_rf = multi_rf_model.predict(X_test)\n",
    "\n",
    "# Initialize dictionaries to store the evaluation metrics for each skill\n",
    "rf_evaluation_metrics = {\n",
    "    'Skill': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': [],\n",
    "    'Confusion Matrix': []\n",
    "}\n",
    "\n",
    "# Loop through each skill (target column) to compute the metrics\n",
    "for skill in valid_skills:\n",
    "    y_pred_skill_rf = y_pred_rf[:, y_valid.columns.get_loc(skill)]\n",
    "    y_true_skill_rf = y_test[skill].values\n",
    "    \n",
    "    acc_rf = accuracy_score(y_true_skill_rf, y_pred_skill_rf)\n",
    "    precision_rf = precision_score(y_true_skill_rf, y_pred_skill_rf, zero_division=0)\n",
    "    recall_rf = recall_score(y_true_skill_rf, y_pred_skill_rf, zero_division=0)\n",
    "    f1_rf = f1_score(y_true_skill_rf, y_pred_skill_rf, zero_division=0)\n",
    "    cm_rf = confusion_matrix(y_true_skill_rf, y_pred_skill_rf)\n",
    "    \n",
    "    # Store the metrics\n",
    "    rf_evaluation_metrics['Skill'].append(skill)\n",
    "    rf_evaluation_metrics['Accuracy'].append(acc_rf)\n",
    "    rf_evaluation_metrics['Precision'].append(precision_rf)\n",
    "    rf_evaluation_metrics['Recall'].append(recall_rf)\n",
    "    rf_evaluation_metrics['F1 Score'].append(f1_rf)\n",
    "    rf_evaluation_metrics['Confusion Matrix'].append(cm_rf)\n",
    "\n",
    "# Convert the metrics dictionary to a DataFrame for easy viewing\n",
    "rf_evaluation_df = pd.DataFrame(rf_evaluation_metrics)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(rf_evaluation_df[['Skill', 'Accuracy', 'Precision', 'Recall', 'F1 Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f45dbb7-fe32-450f-93ee-dbc027d66375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use class_weight='balanced' to help with the imbalanced classes.\n",
    "# Instantiate the Random Forest model with class_weight='balanced'\n",
    "rf_model_balanced = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "\n",
    "# Wrap it in MultiOutputClassifier to handle multi-label classification\n",
    "multi_rf_model_balanced = MultiOutputClassifier(rf_model_balanced, n_jobs=-1)\n",
    "\n",
    "# Train the Random Forest model on the training data\n",
    "multi_rf_model_balanced.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for each skill on the test set\n",
    "y_pred_rf_balanced = multi_rf_model_balanced.predict(X_test)\n",
    "\n",
    "# Initialize a dictionary to store the evaluation metrics\n",
    "rf_evaluation_metrics = {\n",
    "    'Skill': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': [],\n",
    "    'Confusion Matrix': []\n",
    "}\n",
    "\n",
    "# Loop through each skill (target column) to compute the metrics\n",
    "valid_skills = y_train.columns  # Assuming your y_train DataFrame contains skill names as columns\n",
    "for skill in valid_skills:\n",
    "    y_pred_skill_rf = y_pred_rf_balanced[:, y_test.columns.get_loc(skill)]\n",
    "    y_true_skill_rf = y_test[skill].values\n",
    "    \n",
    "    # Calculate metrics for the current skill\n",
    "    acc_rf = accuracy_score(y_true_skill_rf, y_pred_skill_rf)\n",
    "    precision_rf = precision_score(y_true_skill_rf, y_pred_skill_rf, zero_division=0)\n",
    "    recall_rf = recall_score(y_true_skill_rf, y_pred_skill_rf, zero_division=0)\n",
    "    f1_rf = f1_score(y_true_skill_rf, y_pred_skill_rf, zero_division=0)\n",
    "    cm_rf = confusion_matrix(y_true_skill_rf, y_pred_skill_rf)\n",
    "    \n",
    "    # Store the metrics in the dictionary\n",
    "    rf_evaluation_metrics['Skill'].append(skill)\n",
    "    rf_evaluation_metrics['Accuracy'].append(acc_rf)\n",
    "    rf_evaluation_metrics['Precision'].append(precision_rf)\n",
    "    rf_evaluation_metrics['Recall'].append(recall_rf)\n",
    "    rf_evaluation_metrics['F1 Score'].append(f1_rf)\n",
    "    rf_evaluation_metrics['Confusion Matrix'].append(cm_rf)\n",
    "\n",
    "# Convert the metrics dictionary to a DataFrame for easy viewing\n",
    "rf_evaluation_df = pd.DataFrame(rf_evaluation_metrics)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(rf_evaluation_df[['Skill', 'Accuracy', 'Precision', 'Recall', 'F1 Score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7312fd-012d-4c10-922a-71e12395fb71",
   "metadata": {},
   "source": [
    "<h3>Conclusion:</h3>\n",
    "The tuned Random Forest model with class balancing generally shows improvements in recall and F1 scores, particularly for difficult-to-predict classes. The accuracy remains stable overall. However, precision dropped for a few cases, possibly due to the class balancing leading to more false positives. The model performed better for skills like Target_Spark, Target_NLP, and Target_Data Mining, but it still struggles with certain skills like Target_Azure, Target_Keras, and Target_Power BI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0968d-9064-4435-abb1-0c62dc8229bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a function to clean feature names by replacing special characters with underscores\n",
    "def clean_feature_names(df):\n",
    "    df.columns = df.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)  # Replace special characters with '_'\n",
    "    return df\n",
    "\n",
    "# Clean feature names in both training and test sets\n",
    "X_train_cleaned = clean_feature_names(X_train)\n",
    "X_test_cleaned = clean_feature_names(X_test)\n",
    "\n",
    "# Define the LightGBM model\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', objective='binary', random_state=42)\n",
    "\n",
    "# Wrap it in MultiOutputClassifier to handle multi-label classification\n",
    "multi_lgb_model = MultiOutputClassifier(lgb_model, n_jobs=-1)\n",
    "\n",
    "# Train the LightGBM model on the cleaned training data\n",
    "multi_lgb_model.fit(X_train_cleaned, y_train)\n",
    "\n",
    "# Make predictions for each skill on the cleaned test set\n",
    "y_pred_lgb = multi_lgb_model.predict(X_test_cleaned)\n",
    "\n",
    "# Step 7: Initialize dictionaries to store the evaluation metrics for each skill\n",
    "lgb_evaluation_metrics = {\n",
    "    'Skill': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': [],\n",
    "    'Confusion Matrix': []\n",
    "}\n",
    "\n",
    "# Loop through each skill (target column) to compute the metrics\n",
    "for skill in valid_skills:\n",
    "    y_pred_skill_lgb = y_pred_lgb[:, y_valid.columns.get_loc(skill)]\n",
    "    y_true_skill_lgb = y_test[skill].values\n",
    "    \n",
    "    acc_lgb = accuracy_score(y_true_skill_lgb, y_pred_skill_lgb)\n",
    "    precision_lgb = precision_score(y_true_skill_lgb, y_pred_skill_lgb, zero_division=0)\n",
    "    recall_lgb = recall_score(y_true_skill_lgb, y_pred_skill_lgb, zero_division=0)\n",
    "    f1_lgb = f1_score(y_true_skill_lgb, y_pred_skill_lgb, zero_division=0)\n",
    "    cm_lgb = confusion_matrix(y_true_skill_lgb, y_pred_skill_lgb)\n",
    "    \n",
    "    # Store the metrics\n",
    "    lgb_evaluation_metrics['Skill'].append(skill)\n",
    "    lgb_evaluation_metrics['Accuracy'].append(acc_lgb)\n",
    "    lgb_evaluation_metrics['Precision'].append(precision_lgb)\n",
    "    lgb_evaluation_metrics['Recall'].append(recall_lgb)\n",
    "    lgb_evaluation_metrics['F1 Score'].append(f1_lgb)\n",
    "    lgb_evaluation_metrics['Confusion Matrix'].append(cm_lgb)\n",
    "\n",
    "# Convert the metrics dictionary to a DataFrame for easy viewing\n",
    "lgb_evaluation_df = pd.DataFrame(lgb_evaluation_metrics)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(lgb_evaluation_df[['Skill', 'Accuracy', 'Precision', 'Recall', 'F1 Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626105d3-2ba3-4c05-a101-2516f5c7a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff33656-a8b0-4c03-998d-a204b7a9df03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Define the individual base models (with class_weight for imbalance handling)\n",
    "lgb_model_2 = lgb.LGBMClassifier(boosting_type='gbdt', objective='binary', class_weight='balanced', random_state=42)\n",
    "xgb_model = XGBClassifier(objective='binary:logistic', random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Define the meta-model (Logistic Regression in this case) for stacking\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Define the stacking classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lgb', lgb_model_2),\n",
    "        ('xgb', xgb_model)\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Wrap it in MultiOutputClassifier\n",
    "multi_output_stacking_model = MultiOutputClassifier(stacking_model, n_jobs=-1)\n",
    "\n",
    "# Hyperparameter grid for LightGBM\n",
    "lgb_param_grid = {\n",
    "    'estimator__lgb__n_estimators': [50, 100, 200],\n",
    "    'estimator__lgb__learning_rate': [0.01, 0.1, 0.3],\n",
    "    'estimator__lgb__num_leaves': [20, 31, 40],\n",
    "    'estimator__lgb__max_depth': [-1, 5, 10],\n",
    "    'estimator__lgb__min_child_samples': [10, 20, 30],\n",
    "    'estimator__lgb__subsample': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Hyperparameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'estimator__xgb__n_estimators': [50, 100, 200],\n",
    "    'estimator__xgb__learning_rate': [0.01, 0.1, 0.3],\n",
    "    'estimator__xgb__max_depth': [3, 5, 7],\n",
    "    'estimator__xgb__min_child_weight': [1, 5, 10],\n",
    "    'estimator__xgb__subsample': [0.7, 0.8, 1.0],\n",
    "    'estimator__xgb__colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Hyperparameter grid for Logistic Regression (meta-model)\n",
    "lr_param_grid = {\n",
    "    'estimator__final_estimator__C': np.logspace(-4, 4, 10),\n",
    "    'estimator__final_estimator__penalty': ['l2'],\n",
    "    'estimator__final_estimator__solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Combine the parameter grids into a search space\n",
    "param_grid = {**lgb_param_grid, **xgb_param_grid, **lr_param_grid}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(multi_output_stacking_model, param_grid, n_iter=50, scoring='f1_weighted', cv=3, verbose=1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the model with hyperparameter tuning\n",
    "random_search.fit(X_train_cleaned, y_train)\n",
    "\n",
    "# Print the best parameters found by RandomizedSearchCV\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Make predictions with the tuned model\n",
    "y_pred_stacked_tuned = random_search.predict(X_test_cleaned)\n",
    "\n",
    "# Initialize dictionaries to store the evaluation metrics for each skill\n",
    "stacked_evaluation_metrics = {\n",
    "    'Skill': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': [],\n",
    "    'Confusion Matrix': []\n",
    "}\n",
    "\n",
    "# Assuming `valid_skills` is the list of column names representing each skill\n",
    "for i, skill in enumerate(valid_skills):  # Iterate over the skills\n",
    "    # Get predictions and true values for the current skill\n",
    "    y_pred_skill_stacked = y_pred_stacked[:, i]  # Access the i-th skill's predictions\n",
    "    y_true_skill_stacked = y_test[:, i]  # Access the i-th skill's true values\n",
    "\n",
    "    # Calculate metrics for the current skill\n",
    "    acc_stacked = accuracy_score(y_true_skill_stacked, y_pred_skill_stacked)\n",
    "    precision_stacked = precision_score(y_true_skill_stacked, y_pred_skill_stacked, zero_division=0)\n",
    "    recall_stacked = recall_score(y_true_skill_stacked, y_pred_skill_stacked, zero_division=0)\n",
    "    f1_stacked = f1_score(y_true_skill_stacked, y_pred_skill_stacked, zero_division=0)\n",
    "    cm_stacked = confusion_matrix(y_true_skill_stacked, y_pred_skill_stacked)\n",
    "\n",
    "    # Store the metrics\n",
    "    stacked_evaluation_metrics['Skill'].append(skill)\n",
    "    stacked_evaluation_metrics['Accuracy'].append(acc_stacked)\n",
    "    stacked_evaluation_metrics['Precision'].append(precision_stacked)\n",
    "    stacked_evaluation_metrics['Recall'].append(recall_stacked)\n",
    "    stacked_evaluation_metrics['F1 Score'].append(f1_stacked)\n",
    "    stacked_evaluation_metrics['Confusion Matrix'].append(cm_stacked)\n",
    "\n",
    "# Convert the metrics dictionary to a DataFrame for easy viewing\n",
    "stacked_evaluation_df = pd.DataFrame(stacked_evaluation_metrics)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(stacked_evaluation_df[['Skill', 'Accuracy', 'Precision', 'Recall', 'F1 Score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6362d-f441-48d0-95cf-301d7db58fee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the stacking classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lgb', lgb_model_2),\n",
    "        ('xgb', xgb_model)\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Wrap the stacking model in MultiOutputClassifier\n",
    "multi_output_stacking_model = MultiOutputClassifier(stacking_model, n_jobs=-1)\n",
    "\n",
    "# Hyperparameter grid for LightGBM\n",
    "lgb_param_grid = {\n",
    "    'estimator__lgb__n_estimators': [50, 100, 200],\n",
    "    'estimator__lgb__learning_rate': [0.01, 0.1, 0.3],\n",
    "    'estimator__lgb__num_leaves': [20, 31, 40],\n",
    "    'estimator__lgb__max_depth': [-1, 5, 10],\n",
    "    'estimator__lgb__min_child_samples': [10, 20, 30],\n",
    "    'estimator__lgb__subsample': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Hyperparameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'estimator__xgb__n_estimators': [50, 100, 200],\n",
    "    'estimator__xgb__learning_rate': [0.01, 0.1, 0.3],\n",
    "    'estimator__xgb__max_depth': [3, 5, 7],\n",
    "    'estimator__xgb__min_child_weight': [1, 5, 10],\n",
    "    'estimator__xgb__subsample': [0.7, 0.8, 1.0],\n",
    "    'estimator__xgb__colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Hyperparameter grid for Logistic Regression (meta-model)\n",
    "lr_param_grid = {\n",
    "    'estimator__final_estimator__C': np.logspace(-4, 4, 10),\n",
    "    'estimator__final_estimator__penalty': ['l2'],\n",
    "    'estimator__final_estimator__solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Combine the parameter grids into a single search space\n",
    "param_grid = {**lgb_param_grid, **xgb_param_grid, **lr_param_grid}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(multi_output_stacking_model, param_grid, n_iter=50, scoring='f1_weighted', cv=3, verbose=1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the model with hyperparameter tuning\n",
    "random_search.fit(X_train_cleaned, y_train)\n",
    "\n",
    "# Print the best parameters found by RandomizedSearchCV\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Make predictions with the tuned model\n",
    "y_pred_stacked_tuned = random_search.predict(X_test_cleaned)\n",
    "\n",
    "# Initialize dictionaries to store the evaluation metrics for each skill\n",
    "stacked_evaluation_metrics = {\n",
    "    'Skill': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': [],\n",
    "    'Confusion Matrix': []\n",
    "}\n",
    "\n",
    "# Assuming `valid_skills` is the list of column names representing each skill\n",
    "for i, skill in enumerate(valid_skills):  # Iterate over the skills\n",
    "    # Get predictions and true values for the current skill\n",
    "    y_pred_skill_stacked = y_pred_stacked_tuned[:, i]  # Access the i-th skill's predictions\n",
    "    y_true_skill_stacked = y_test[:, i]  # Access the i-th skill's true values\n",
    "\n",
    "    # Calculate metrics for the current skill\n",
    "    acc_stacked = accuracy_score(y_true_skill_stacked, y_pred_skill_stacked)\n",
    "    precision_stacked = precision_score(y_true_skill_stacked, y_pred_skill_stacked, zero_division=0)\n",
    "    recall_stacked = recall_score(y_true_skill_stacked, y_pred_skill_stacked, zero_division=0)\n",
    "    f1_stacked = f1_score(y_true_skill_stacked, y_pred_skill_stacked, zero_division=0)\n",
    "    cm_stacked = confusion_matrix(y_true_skill_stacked, y_pred_skill_stacked)\n",
    "\n",
    "    # Store the metrics\n",
    "    stacked_evaluation_metrics['Skill'].append(skill)\n",
    "    stacked_evaluation_metrics['Accuracy'].append(acc_stacked)\n",
    "    stacked_evaluation_metrics['Precision'].append(precision_stacked)\n",
    "    stacked_evaluation_metrics['Recall'].append(recall_stacked)\n",
    "    stacked_evaluation_metrics['F1 Score'].append(f1_stacked)\n",
    "    stacked_evaluation_metrics['Confusion Matrix'].append(cm_stacked)\n",
    "\n",
    "# Convert the metrics dictionary to a DataFrame for easy viewing\n",
    "stacked_evaluation_df_2 = pd.DataFrame(stacked_evaluation_metrics)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(stacked_evaluation_df_2[['Skill', 'Accuracy', 'Precision', 'Recall', 'F1 Score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5545fd-8ef6-4c1c-9e6a-6a09bdfb3e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stacked_evaluation_df_2[['Skill', 'Accuracy', 'Precision', 'Recall', 'F1 Score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24d7c3-9441-4cab-be99-691f53e1dee4",
   "metadata": {},
   "source": [
    "<h3>Conclusion:</h3>\n",
    "\n",
    "The tuned model shows some overall improvements, particularly in skills like Target_Deep Learning, but also experiences minor trade-offs in precision and recall for certain skills. The model has become more balanced, with some skills seeing better F1 scores at the expense of slight declines in precision or recall. Further fine-tuning could address these marginal trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c95550e-9dcf-4397-ade2-45a19233ffab",
   "metadata": {},
   "source": [
    "<h3>Summary of all 3 models:</h3>\n",
    "\n",
    "<b>Logistic Regression:</b> Best for environments where simplicity, speed, and interpretability are critical, but it does not perform well for complex, non-linear problems.\n",
    "\n",
    "<b>Random Forest:</b> A strong middle ground, offering good performance for a wide variety of tasks while being more scalable and efficient than the stacked model. It balances complexity and predictive power, making it a good general-purpose model, especially when computational resources are available.\n",
    "\n",
    "<b>Stacked Model (LightGBM + XGBoost):</b> Best for extracting maximum performance in complex problems, particularly for tasks like Deep Learning and Machine Learning where it showed improvement. However, the high computational cost and maintenance requirements make it suitable for environments with abundant computational resources and the ability to handle model complexity.\n",
    "\n",
    "<h3>Recommendation:</h3>\n",
    "\n",
    "For most use cases, especially where performance and computational efficiency are both important, **Random Forest** is the best overall option. It provides a good balance of performance, scalability, and maintenance cost.\n",
    "**Stacked Model** is preferable when the highest predictive accuracy is required, particularly for complex non-linear problems, and when computational resources and time for tuning are not limiting factors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
